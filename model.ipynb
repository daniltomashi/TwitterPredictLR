{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d638a679",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "import neattext as ntx\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, InputLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bdf84c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_csv('datasets/data.csv')\n",
    "data = pd.read_csv('datasets/less_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6716a37f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text_id</th>\n",
       "      <th>text</th>\n",
       "      <th>reply_count</th>\n",
       "      <th>like_count</th>\n",
       "      <th>lang</th>\n",
       "      <th>type</th>\n",
       "      <th>post_created_at</th>\n",
       "      <th>followers_count</th>\n",
       "      <th>acc_created_at</th>\n",
       "      <th>verified</th>\n",
       "      <th>statuses_count</th>\n",
       "      <th>friends_count</th>\n",
       "      <th>info_tweet</th>\n",
       "      <th>retweets</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>userhandles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>67403135</td>\n",
       "      <td>212953800187711489</td>\n",
       "      <td>Pretty interesting: http://t.co/RtlMsOTf</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "      <td>tweet</td>\n",
       "      <td>2012-06-13 17:05:17+00:00</td>\n",
       "      <td>166.0</td>\n",
       "      <td>2009-08-20 19:40:52+00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>89.0</td>\n",
       "      <td>1233.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>pretty interesting: http://tco/rtlmsotf</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>57357366</td>\n",
       "      <td>686558302503854080</td>\n",
       "      <td>WIN a $50 Walmart Gift Card https://t.co/pXRjf...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>tweet</td>\n",
       "      <td>2016-01-11 14:40:29+00:00</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2009-07-16 15:10:05+00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>787.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>win $50 walmart gift card https://tco/pxrjfawizk</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23867915</td>\n",
       "      <td>1305300491417591809</td>\n",
       "      <td>Cary Elwes and Robin Wright got better looking...</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>en</td>\n",
       "      <td>tweet</td>\n",
       "      <td>2020-09-14 00:21:03+00:00</td>\n",
       "      <td>57.0</td>\n",
       "      <td>2009-03-12 01:19:00+00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>1836.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>cary elw robin wright got bet look ag husband ...</td>\n",
       "      <td>['#PrincessBrideReunion']</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>26296558</td>\n",
       "      <td>674274040937185280</td>\n",
       "      <td>#Thrive4Tots Help Children in need by ord #Thr...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>tweet</td>\n",
       "      <td>2015-12-08 17:07:13+00:00</td>\n",
       "      <td>41.0</td>\n",
       "      <td>2009-03-24 18:55:29+00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>403.0</td>\n",
       "      <td>196.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>#thrive4tots help childr nee ord #thrive prtio...</td>\n",
       "      <td>['#Thrive4Tots', '#Thrive', '#TOYS4TOTS']</td>\n",
       "      <td>['@snooki']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14975460</td>\n",
       "      <td>1270346530889830402</td>\n",
       "      <td>Aunt Candace’s Shanty has a good ring it, nay?...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>tweet</td>\n",
       "      <td>2020-06-09 13:26:29+00:00</td>\n",
       "      <td>423.0</td>\n",
       "      <td>2008-06-01 22:29:21+00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>2005.0</td>\n",
       "      <td>223.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>aunt candac shanty good ring it nay publ ow bi...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41932</th>\n",
       "      <td>32921025</td>\n",
       "      <td>1380491039216123907</td>\n",
       "      <td>French senate: Hijab Ban France https://t.co/k...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>tweet</td>\n",
       "      <td>2021-04-09 12:01:26+00:00</td>\n",
       "      <td>704.0</td>\n",
       "      <td>2009-04-18 15:52:22+00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>19212.0</td>\n",
       "      <td>641.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>french senate: hijab ban frant https://tco/kqt...</td>\n",
       "      <td>[]</td>\n",
       "      <td>['@Change']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41933</th>\n",
       "      <td>71986939</td>\n",
       "      <td>24076025278</td>\n",
       "      <td>Check out this awesome site: Insidmal on I - h...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>tweet</td>\n",
       "      <td>2010-09-10 04:24:08+00:00</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2009-09-06 07:06:18+00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>98.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>check awesom site: insidm http://sweevacom/go/...</td>\n",
       "      <td>['#Sweeva']</td>\n",
       "      <td>['@insidmaldesign']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41934</th>\n",
       "      <td>14792201</td>\n",
       "      <td>96418154426810368</td>\n",
       "      <td>Man I will say I really don't understand relat...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>tweet</td>\n",
       "      <td>2011-07-28 03:14:14+00:00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2008-05-15 23:48:00+00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>man understand rel</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41935</th>\n",
       "      <td>97777065</td>\n",
       "      <td>1549315866122133504</td>\n",
       "      <td>Galway RNLI rescues six people from water afte...</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>en</td>\n",
       "      <td>tweet</td>\n",
       "      <td>2022-07-19 08:51:02+00:00</td>\n",
       "      <td>2432.0</td>\n",
       "      <td>2009-12-19 00:04:40+00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>3714.0</td>\n",
       "      <td>182.0</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>galway rnli rescu peopl wat group get cut tid ...</td>\n",
       "      <td>['#RNLI', '#Galway', '#Lifeboat']</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41936</th>\n",
       "      <td>74730744</td>\n",
       "      <td>72694168127930368</td>\n",
       "      <td>will serve as faculty for this cutting-edge we...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>tweet</td>\n",
       "      <td>2011-05-23 16:03:34+00:00</td>\n",
       "      <td>19.0</td>\n",
       "      <td>2009-09-16 13:31:55+00:00</td>\n",
       "      <td>False</td>\n",
       "      <td>27.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>serv facul cuttingedg weeklong institut re: em...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>41937 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id              text_id  \\\n",
       "0      67403135   212953800187711489   \n",
       "1      57357366   686558302503854080   \n",
       "2      23867915  1305300491417591809   \n",
       "3      26296558   674274040937185280   \n",
       "4      14975460  1270346530889830402   \n",
       "...         ...                  ...   \n",
       "41932  32921025  1380491039216123907   \n",
       "41933  71986939          24076025278   \n",
       "41934  14792201    96418154426810368   \n",
       "41935  97777065  1549315866122133504   \n",
       "41936  74730744    72694168127930368   \n",
       "\n",
       "                                                    text  reply_count  \\\n",
       "0               Pretty interesting: http://t.co/RtlMsOTf            1   \n",
       "1      WIN a $50 Walmart Gift Card https://t.co/pXRjf...            0   \n",
       "2      Cary Elwes and Robin Wright got better looking...            0   \n",
       "3      #Thrive4Tots Help Children in need by ord #Thr...            0   \n",
       "4      Aunt Candace’s Shanty has a good ring it, nay?...            0   \n",
       "...                                                  ...          ...   \n",
       "41932  French senate: Hijab Ban France https://t.co/k...            0   \n",
       "41933  Check out this awesome site: Insidmal on I - h...            0   \n",
       "41934  Man I will say I really don't understand relat...            0   \n",
       "41935  Galway RNLI rescues six people from water afte...            0   \n",
       "41936  will serve as faculty for this cutting-edge we...            0   \n",
       "\n",
       "       like_count lang   type            post_created_at  followers_count  \\\n",
       "0               1   en  tweet  2012-06-13 17:05:17+00:00            166.0   \n",
       "1               0   en  tweet  2016-01-11 14:40:29+00:00             11.0   \n",
       "2              10   en  tweet  2020-09-14 00:21:03+00:00             57.0   \n",
       "3               0   en  tweet  2015-12-08 17:07:13+00:00             41.0   \n",
       "4               0   en  tweet  2020-06-09 13:26:29+00:00            423.0   \n",
       "...           ...  ...    ...                        ...              ...   \n",
       "41932           0   en  tweet  2021-04-09 12:01:26+00:00            704.0   \n",
       "41933           0   en  tweet  2010-09-10 04:24:08+00:00              6.0   \n",
       "41934           0   en  tweet  2011-07-28 03:14:14+00:00              2.0   \n",
       "41935          14   en  tweet  2022-07-19 08:51:02+00:00           2432.0   \n",
       "41936           0   en  tweet  2011-05-23 16:03:34+00:00             19.0   \n",
       "\n",
       "                  acc_created_at  verified  statuses_count  friends_count  \\\n",
       "0      2009-08-20 19:40:52+00:00     False            89.0         1233.0   \n",
       "1      2009-07-16 15:10:05+00:00     False           787.0           40.0   \n",
       "2      2009-03-12 01:19:00+00:00     False          1836.0           60.0   \n",
       "3      2009-03-24 18:55:29+00:00     False           403.0          196.0   \n",
       "4      2008-06-01 22:29:21+00:00     False          2005.0          223.0   \n",
       "...                          ...       ...             ...            ...   \n",
       "41932  2009-04-18 15:52:22+00:00     False         19212.0          641.0   \n",
       "41933  2009-09-06 07:06:18+00:00     False            98.0            4.0   \n",
       "41934  2008-05-15 23:48:00+00:00     False            16.0            0.0   \n",
       "41935  2009-12-19 00:04:40+00:00     False          3714.0          182.0   \n",
       "41936  2009-09-16 13:31:55+00:00     False            27.0           12.0   \n",
       "\n",
       "       info_tweet  retweets  \\\n",
       "0               2         0   \n",
       "1               0         0   \n",
       "2              10         0   \n",
       "3               0         0   \n",
       "4               0         0   \n",
       "...           ...       ...   \n",
       "41932           0         0   \n",
       "41933           0         0   \n",
       "41934           0         0   \n",
       "41935          17         3   \n",
       "41936           1         1   \n",
       "\n",
       "                                              clean_text  \\\n",
       "0                pretty interesting: http://tco/rtlmsotf   \n",
       "1       win $50 walmart gift card https://tco/pxrjfawizk   \n",
       "2      cary elw robin wright got bet look ag husband ...   \n",
       "3      #thrive4tots help childr nee ord #thrive prtio...   \n",
       "4      aunt candac shanty good ring it nay publ ow bi...   \n",
       "...                                                  ...   \n",
       "41932  french senate: hijab ban frant https://tco/kqt...   \n",
       "41933  check awesom site: insidm http://sweevacom/go/...   \n",
       "41934                                 man understand rel   \n",
       "41935  galway rnli rescu peopl wat group get cut tid ...   \n",
       "41936  serv facul cuttingedg weeklong institut re: em...   \n",
       "\n",
       "                                        hashtags          userhandles  \n",
       "0                                             []                   []  \n",
       "1                                             []                   []  \n",
       "2                      ['#PrincessBrideReunion']                   []  \n",
       "3      ['#Thrive4Tots', '#Thrive', '#TOYS4TOTS']          ['@snooki']  \n",
       "4                                             []                   []  \n",
       "...                                          ...                  ...  \n",
       "41932                                         []          ['@Change']  \n",
       "41933                                ['#Sweeva']  ['@insidmaldesign']  \n",
       "41934                                         []                   []  \n",
       "41935          ['#RNLI', '#Galway', '#Lifeboat']                   []  \n",
       "41936                                         []                   []  \n",
       "\n",
       "[41937 rows x 18 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ecb6284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data['retweets'] = data['retweet_count'] + data['quote_count']\n",
    "# data.drop(['retweet_count', 'quote_count'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a618cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def what_class(num):\n",
    "    if num >= 0 and num < 10:\n",
    "        return '0-10'\n",
    "    elif num >= 10 and num < 60:\n",
    "        return '10-60'\n",
    "    elif num >= 60 and num < 100:\n",
    "        return '60-100'\n",
    "    elif num >= 100 and num < 200:\n",
    "        return '100-200'\n",
    "    elif num >= 200 and num < 500:\n",
    "        return '200-500'\n",
    "    elif num >= 500 and num < 1000:\n",
    "        return '500-1000'\n",
    "    elif num >= 1000 and num < 2000:\n",
    "        return '1000-2000'\n",
    "    elif num >= 2000 and num < 5000:\n",
    "        return '2000-5000'\n",
    "    elif num >= 5000 and num < 10000:\n",
    "        return '5000-10000'\n",
    "    elif num >= 10000 and num < 20000:\n",
    "        return '10000-20000'\n",
    "    elif num >= 20000 and num < 50000:\n",
    "        return '20000-50000'\n",
    "    elif num >= 50000 and num < 100000:\n",
    "        return '50000-100000'\n",
    "    else:\n",
    "        return '100000+'\n",
    "\n",
    "data['likes_class'] = data['like_count'].apply(lambda x: what_class(x))\n",
    "data['retweets_class'] = data['retweets'].apply(lambda x: what_class(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0baa0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_class_to_num = {'0-10':1, '10-60':2, '60-100':3, '100-200':4, '200-500':5, '500-1000':6, '1000-2000':7,\\\n",
    "                       '2000-5000':8, '5000-10000':9, '10000-20000':10, '20000-50000':11, '50000-100000':12, \\\n",
    "                        '100000+':13}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9135244c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['likes_class'] = data['likes_class'].apply(lambda x: convert_class_to_num[x])\n",
    "data['retweets_class'] = data['retweets_class'].apply(lambda x: convert_class_to_num[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1fcb4c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop_values = np.random.choice(data[(data['info_tweet'] == 0) & (data['followers_count'] < 10)]['text_id'], 4000)\n",
    "drop_values = np.random.choice(data[data['info_tweet'] < 10]['text_id'], 8000)\n",
    "\n",
    "data = data[~data['text_id'].isin(drop_values)]\n",
    "data.index = range(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe7a5b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = data[data['type'] == 'tweet']\n",
    "# data = data[data['lang'] == 'en']\n",
    "# data.index = range(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f67f79a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# stemmer = LancasterStemmer()\n",
    "\n",
    "# def remove_emojis(data):\n",
    "#     emoj = re.compile(\"[\"\n",
    "#         u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "#         u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "#         u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "#         u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "#         u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "#         u\"\\U00002702-\\U000027B0\"\n",
    "#         u\"\\U00002702-\\U000027B0\"\n",
    "#         u\"\\U000024C2-\\U0001F251\"\n",
    "#         u\"\\U0001f926-\\U0001f937\"\n",
    "#         u\"\\U00010000-\\U0010ffff\"\n",
    "#         u\"\\u2640-\\u2642\" \n",
    "#         u\"\\u2600-\\u2B55\"\n",
    "#         u\"\\u200d\"\n",
    "#         u\"\\u23cf\"\n",
    "#         u\"\\u23e9\"\n",
    "#         u\"\\u231a\"\n",
    "#         u\"\\ufe0f\"  # dingbats\n",
    "#         u\"\\u3030\"\n",
    "#                       \"]+\", re.UNICODE)\n",
    "#     return re.sub(emoj, '', data)\n",
    "\n",
    "# def stem_sentence(text):\n",
    "#     text = [stemmer.stem(word) for word in text.split()]\n",
    "    \n",
    "#     return ' '.join(text)\n",
    "    \n",
    "    \n",
    "# data['clean_text'] = data['text'].apply(lambda x: ntx.remove_stopwords(x))\n",
    "# data['clean_text'] = data['clean_text'].apply(lambda x: ntx.remove_punctuations(x))\n",
    "# data['clean_text'] = data['clean_text'].apply(lambda x: ntx.remove_multiple_spaces(x))\n",
    "# data['clean_text'] = data['clean_text'].apply(lambda x: x.encode('ascii', 'ignore').decode('ascii'))\n",
    "# data['clean_text'] = data['clean_text'].apply(lambda x: remove_emojis(x))\n",
    "\n",
    "# data['hashtags'] = data['clean_text'].apply(lambda x: ntx.extract_hashtags(x))\n",
    "# data['urls'] = data['clean_text'].apply(lambda x: ntx.extract_urls(x))\n",
    "# data['userhandles'] = data['clean_text'].apply(lambda x: ntx.extract_userhandles(x))\n",
    "# # data['clean_text'] = data['clean_text'].apply(lambda x: ntx.remove_hashtags(x))\n",
    "# data['clean_text'] = data['clean_text'].apply(lambda x: ntx.remove_urls(x))\n",
    "# data['clean_text'] = data['clean_text'].apply(lambda x: ntx.remove_userhandles(x))\n",
    "\n",
    "# data['clean_text'] = data['clean_text'].apply(lambda x: stem_sentence(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee966eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(data['clean_text'][0])\n",
    "# print(data['text'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb235f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def number_in_text(text, number):\n",
    "#     for i in number:\n",
    "#         if i in text:\n",
    "#             return True\n",
    "#     return False\n",
    "\n",
    "# a = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "# print('Clean text:', data['clean_text'].apply(lambda x: number_in_text(x,a)).sum(), 'sentences with numbers')\n",
    "# print('Text:', data['text'].apply(lambda x: number_in_text(x,a)).sum(), 'sentences with numbers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "87062154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# less_data = pd.DataFrame(columns=data.columns)\n",
    "\n",
    "# ind = np.random.randint(0, data.index[-1]+1, 42000)\n",
    "# less_data = data.loc[ind]\n",
    "\n",
    "# less_data.to_csv('datasets/less_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2068ef97",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data['likes_class'].copy()\n",
    "y = y.to_numpy().astype(np.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "177735bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "########## TEXT\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "# vectorizer = CountVectorizer(binary=True)\n",
    "\n",
    "vectorized_text = vectorizer.fit_transform(data['clean_text'])\n",
    "vectorized_text = vectorized_text.astype(np.float32)\n",
    "# vectorized_text = vectorized_text.astype(np.int32)\n",
    "vectorized_text = vectorized_text.toarray()\n",
    "\n",
    "\n",
    "vectorized_text_with_followers = np.insert(vectorized_text, 0, data['followers_count'], axis=1) \n",
    "del vectorized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "94afa53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to_input = vectorized_text_with_followers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e872c1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "to_input = scaler.fit_transform(vectorized_text_with_followers)\n",
    "to_input = to_input.astype(np.float16)\n",
    "del vectorized_text_with_followers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a10f50cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to_input_train, to_input_test, y_train, y_test = train_test_split(to_input, y, test_size=.2)\n",
    "# del to_input\n",
    "# del y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac7cb6b",
   "metadata": {},
   "source": [
    "# CLASSIFICATION MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c6098a01",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/danilo/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/danilo/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/danilo/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/danilo/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/home/danilo/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "for i in range(5):\n",
    "    to_input_train, to_input_test, y_train, y_test = train_test_split(to_input, y, test_size=.2)\n",
    "    del to_input\n",
    "    del y\n",
    "\n",
    "    model = LogisticRegression()\n",
    "    model.fit(to_input_train, y_train)\n",
    "    scores.append(model.score(to_input_test, y_test))\n",
    "    \n",
    "    to_input = np.concatenate((to_input_train, to_input_test))\n",
    "    y = np.concatenate((y_train, y_test))\n",
    "    \n",
    "    del to_input_train\n",
    "    del to_input_test\n",
    "    del y_train\n",
    "    del y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "97e24246",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8889169675090253"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(scores) / len(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e3ef20a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.]\n",
      "3.0\n"
     ]
    }
   ],
   "source": [
    "# print(np.where(y == 3))\n",
    "n = 18\n",
    "\n",
    "print(model.predict(to_input[n][np.newaxis, :]))\n",
    "print(y[n])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e5b870",
   "metadata": {},
   "source": [
    "# NEURAL NETWORK BAD IDEA, BECAUSE WE NEED SUCH A POWER COMPUTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "344d6bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Sequential()\n",
    "# model.add(LSTM(50, return_sequences=True, input_shape=(1,to_input_train.shape[-1])))\n",
    "# model.add(LSTM(50))\n",
    "# model.add(Dense(32))\n",
    "# model.add(Dense(1, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6316d094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d41d5d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fit(to_input_train[:, np.newaxis, :], y_train, batch_size=32, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "993ff723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save('model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e58ecde",
   "metadata": {},
   "source": [
    "## HASHTAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2ded3bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def take_unique_hashtags(ser):\n",
    "#     unique = []\n",
    "#     for lis in ser:\n",
    "#         for hashtag in lis:\n",
    "#             if hashtag not in unique:\n",
    "#                 unique.append(hashtag)\n",
    "    \n",
    "#     return unique\n",
    "\n",
    "# data['hashtags'] = clear_hashtags(data['hashtags'].copy())\n",
    "# unique_hashtags = take_unique_hashtags(data['hashtags'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f2075ee9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ########## HASHTAGS\n",
    "\n",
    "\n",
    "# vectorize_hash = TfidfVectorizer()\n",
    "\n",
    "# # vectorized_hash_fited = vectorize_hash.fit_transform(data['hashtags'])\n",
    "# # vectorized_hash_fited = vectorized_hash_fited.astype(np.float16)\n",
    "# # vectorized_hash_fited = vectorized_hash_fited.toarray()\n",
    "\n",
    "# vectorize_hash.fit(unique_hashtags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "24576cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len_of_hash_uniq = len(vectorize_hash.get_feature_names_out())\n",
    "\n",
    "# def convert_hashtag_to_vector(hashtags, vectorizer, len_of_features):\n",
    "#     ones = np.ones((len_of_features))\n",
    "#     if len(hashtags) == 1:\n",
    "#         ind = np.argmax(vectorizer.transform(hashtags))\n",
    "#         ones[ind] = 1\n",
    "#     elif len(hashtags) >= 2:\n",
    "#         indexes = []\n",
    "#         for hashtag in hashtags:\n",
    "#             indexes.append(np.argmax(vectorizer.transform([hashtag])))\n",
    "#         for index in indexes:\n",
    "#             ones[index] = 1\n",
    "        \n",
    "#     return ones\n",
    "\n",
    "        \n",
    "# # vectorize_hash.transform(['#hello']).toarray()\n",
    "# data['hashtags'] = data['hashtags'].apply(lambda x: convert_hashtag_to_vector(x, vectorize_hash, len_of_hash_uniq))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
